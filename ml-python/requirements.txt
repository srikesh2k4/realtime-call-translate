# Web framework
fastapi==0.111.0
uvicorn==0.30.1

# OpenAI client (compatible with httpx below)
openai==1.30.5
httpx==0.24.1

# ML / NLP
torch>=2.0,<2.3
transformers>=4.38,<4.45
huggingface_hub>=0.20,<0.24

# Utilities
python-dotenv>=1.0



Below is a **complete, chronological, copy-paste-ready command history** covering **everything used to deploy your ML FastAPI worker** on **Ubuntu 24.04 with Python 3.10**.

This is suitable for **documentation, re-deployment, or automation**.

---

# ðŸ”¹ Phase 1 â€” System preparation

```bash
cat /etc/os-release
```

```bash
apt update
apt install -y software-properties-common curl
```

---

# ðŸ”¹ Phase 2 â€” Install Python 3.10 (Ubuntu 24.04)

```bash
add-apt-repository ppa:deadsnakes/ppa
apt update
```

```bash
apt install -y python3.10 python3.10-venv python3.10-dev
```

```bash
python3.10 --version
```

---

# ðŸ”¹ Phase 3 â€” Virtual environment setup

```bash
deactivate 2>/dev/null
rm -rf venv
```

```bash
python3.10 -m venv venv
source venv/bin/activate
```

```bash
python --version
```

---

# ðŸ”¹ Phase 4 â€” Python tooling upgrade

```bash
pip install --upgrade pip setuptools wheel
```

---

# ðŸ”¹ Phase 5 â€” Fix OpenAI / httpx incompatibility

```bash
pip uninstall -y openai httpx
```

```bash
pip install openai==1.30.5 httpx==0.24.1
```

---

# ðŸ”¹ Phase 6 â€” Install application dependencies

```bash
pip install fastapi uvicorn torch transformers huggingface_hub
```

*(or)*

```bash
pip install -r requirements.txt
```

---

# ðŸ”¹ Phase 7 â€” Run server (foreground test)

```bash
uvicorn worker:app \
  --host 0.0.0.0 \
  --port 9001 \
  --workers 1 \
  --lifespan off
```

---

# ðŸ”¹ Phase 8 â€” Background deployment

```bash
nohup uvicorn worker:app \
  --host 0.0.0.0 \
  --port 9001 \
  --workers 1 \
  --lifespan off \
  > ml.log 2>&1 &
```

---

# ðŸ”¹ Phase 9 â€” Health verification

```bash
curl http://localhost:9001/docs
```

---

# ðŸ”¹ Phase 10 â€” Optional warning suppression

```bash
export PYTHONWARNINGS="ignore::FutureWarning"
```

---

# ðŸ”¹ Phase 11 â€” (Optional) Dependency locking

```bash
pip freeze > requirements.lock
```

---

## âœ… Final Result

* Python **3.10**
* FastAPI worker running on **port 9001**
* Stable ML stack
* No asyncio cancellation errors
* Background-safe deployment

---

## ðŸ“Œ Recommended to save this as

```
deploy-ml-worker.sh
```

If you want, I can:

* Convert this into a **single executable shell script**
* Provide a **Dockerfile**
* Add **systemd service**
* Add **rollback / cleanup commands**

Tell me what you want next.
